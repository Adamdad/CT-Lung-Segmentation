import torchfrom torch import nnimport torch.nn.functional as Fclass DoubleConv(nn.Module):    """(convolution => [BN] => ReLU) * 2"""    def __init__(self, in_channels, out_channels, mid_channels=None):        super().__init__()        if not mid_channels:            mid_channels = out_channels        self.double_conv = nn.Sequential(            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),            nn.BatchNorm2d(mid_channels),            nn.ReLU(inplace=True),            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),            nn.BatchNorm2d(out_channels),            nn.ReLU(inplace=True)        )    def forward(self, x):        return self.double_conv(x)class ResBlock(nn.Module):    def __init__(self, in_channels, out_channels):        super(ResBlock, self).__init__()        self.downsample = nn.Sequential(            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),            nn.BatchNorm2d(out_channels))        self.double_conv = DoubleConv(in_channels, out_channels)        self.down_sample = nn.MaxPool2d(2)        self.relu = nn.ReLU()    def forward(self, x):        identity = self.downsample(x)        out = self.double_conv(x)        out = self.relu(out + identity)        return self.down_sample(out), outclass UpBlock(nn.Module):    def __init__(self, in_channels, out_channels):        super(UpBlock, self).__init__()        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        self.double_conv = DoubleConv(in_channels, out_channels)    def forward(self, down_input, skip_input):        x = self.up_sample(down_input)        # input is CHW        diffY = skip_input.size()[2] - x.size()[2]        diffX = skip_input.size()[3] - x.size()[3]        x = F.pad(x, [diffX // 2, diffX - diffX // 2,                        diffY // 2, diffY - diffY // 2])        x = torch.cat([x, skip_input], dim=1)        return self.double_conv(x)class OutConv(nn.Module):    def __init__(self, in_channels, out_channels):        super(OutConv, self).__init__()        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)    def forward(self, x):        return self.conv(x)class ResUNet(nn.Module):    """    Hybrid solution of resnet blocks and double conv blocks    """    def __init__(self, out_classes=2):        super(ResUNet, self).__init__()        self.down_conv1 = ResBlock(3, 64)        self.down_conv2 = ResBlock(64, 128)        self.down_conv3 = ResBlock(128, 256)        self.down_conv4 = ResBlock(256, 512)        self.double_conv = DoubleConv(512, 1024)        self.up_conv4 = UpBlock(512 + 1024, 512)        self.up_conv3 = UpBlock(256 + 512, 256)        self.up_conv2 = UpBlock(128 + 256, 128)        self.up_conv1 = UpBlock(128 + 64, 64)        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)    def forward(self, x):        x, skip1_out = self.down_conv1(x)        x, skip2_out = self.down_conv2(x)        x, skip3_out = self.down_conv3(x)        x, skip4_out = self.down_conv4(x)        x = self.double_conv(x)        x = self.up_conv4(x, skip4_out)        x = self.up_conv3(x, skip3_out)        x = self.up_conv2(x, skip2_out)        x = self.up_conv1(x, skip1_out)        x = self.conv_last(x)        return x